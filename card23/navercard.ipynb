{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'data': {'cardAdList': {'cardAds': [{'cardAdId': 1530}, {'cardAdId': 10105}, {'cardAdId': 3881}, {'cardAdId': 10285}, {'cardAdId': 10102}, {'cardAdId': 1408}, {'cardAdId': 10179}, {'cardAdId': 10065}, {'cardAdId': 2776}, {'cardAdId': 247}], 'totalSize': 354}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "URL = 'https://card-search.naver.com/graphql'\n",
    "\n",
    "params = {\n",
    "    \"operationName\": \"smartSearch\",\n",
    "    \"variables\": {\n",
    "        'basePayment': 0,\n",
    "        'bizType': 'CPC',\n",
    "        'device': 'pc',\n",
    "        'isRefetch': False,\n",
    "        'maxAnnualFee': 0,\n",
    "        'minAnnualFee': 0,\n",
    "        'pageNo': 1,\n",
    "        'pageSize': 10,\n",
    "        'sortMethod': 'ri'\n",
    "    },\n",
    "    \"query\": '''query smartSearch($basePayment: Int, $bizType: BizType, $device: AdDeviceType, $isRefetch: Boolean, $maxAnnualFee: Int, $minAnnualFee: Int, $pageNo: Int, $pageSize: Int, $sortMethod: SortMethod) {\n",
    "        cardAdList(\n",
    "            basePayment: $basePayment,\n",
    "            bizType: $bizType,\n",
    "            device: $device,\n",
    "            isRefetch: $isRefetch,\n",
    "            maxAnnualFee: $maxAnnualFee,\n",
    "            minAnnualFee: $minAnnualFee,\n",
    "            pageNo: $pageNo,\n",
    "            pageSize: $pageSize,\n",
    "            sortMethod: $sortMethod\n",
    "        ) {\n",
    "            cardAds {\n",
    "                cardAdId\n",
    "                # Add other fields you want to extract here\n",
    "            }\n",
    "            totalSize\n",
    "        }\n",
    "    }'''\n",
    "}\n",
    "\n",
    "response = requests.post(URL, json=params, headers=headers)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'사이트': 1530}, {'사이트': 10105}, {'사이트': 10285}, {'사이트': 3881}, {'사이트': 10102}, {'사이트': 1408}, {'사이트': 10065}, {'사이트': 10179}, {'사이트': 247}, {'사이트': 2776}, {'사이트': 10117}, {'사이트': 1692}, {'사이트': 3857}, {'사이트': 10227}, {'사이트': 10157}, {'사이트': 10141}, {'사이트': 10282}, {'사이트': 277}, {'사이트': 1684}, {'사이트': 115}, {'사이트': 10145}, {'사이트': 10176}, {'사이트': 2332}, {'사이트': 3856}, {'사이트': 10106}, {'사이트': 3418}, {'사이트': 1465}, {'사이트': 10071}, {'사이트': 4047}, {'사이트': 10107}, {'사이트': 1531}, {'사이트': 10284}, {'사이트': 253}, {'사이트': 10184}, {'사이트': 10155}, {'사이트': 10156}, {'사이트': 10126}, {'사이트': 2471}, {'사이트': 10216}, {'사이트': 593}, {'사이트': 10142}, {'사이트': 10146}, {'사이트': 10152}, {'사이트': 10103}, {'사이트': 1361}, {'사이트': 3896}, {'사이트': 10256}, {'사이트': 10246}, {'사이트': 10121}, {'사이트': 2756}, {'사이트': 3676}, {'사이트': 1322}, {'사이트': 10180}, {'사이트': 1418}, {'사이트': 10267}, {'사이트': 10122}, {'사이트': 1316}, {'사이트': 3717}, {'사이트': 10269}, {'사이트': 2484}, {'사이트': 2571}, {'사이트': 2483}, {'사이트': 10268}, {'사이트': 1260}, {'사이트': 2916}, {'사이트': 3456}, {'사이트': 10221}, {'사이트': 10045}, {'사이트': 1768}, {'사이트': 1715}, {'사이트': 10093}, {'사이트': 1202}, {'사이트': 3878}, {'사이트': 3836}, {'사이트': 2225}, {'사이트': 2337}, {'사이트': 111}, {'사이트': 4038}, {'사이트': 10264}, {'사이트': 10219}, {'사이트': 10193}, {'사이트': 10114}, {'사이트': 1385}, {'사이트': 3996}, {'사이트': 10129}, {'사이트': 10261}, {'사이트': 99}, {'사이트': 1030}, {'사이트': 10005}, {'사이트': 3277}, {'사이트': 10187}, {'사이트': 10245}, {'사이트': 10046}, {'사이트': 10112}, {'사이트': 10056}, {'사이트': 2348}, {'사이트': 10265}, {'사이트': 10266}, {'사이트': 10241}, {'사이트': 10013}, {'사이트': 10051}, {'사이트': 3637}, {'사이트': 10215}, {'사이트': 1312}, {'사이트': 10039}, {'사이트': 2615}, {'사이트': 1772}, {'사이트': 3356}, {'사이트': 10124}, {'사이트': 10252}, {'사이트': 2472}, {'사이트': 1362}, {'사이트': 3638}, {'사이트': 1342}, {'사이트': 1946}, {'사이트': 10214}, {'사이트': 2226}, {'사이트': 10220}, {'사이트': 10134}, {'사이트': 10226}, {'사이트': 10050}, {'사이트': 2856}, {'사이트': 3578}, {'사이트': 2857}, {'사이트': 710}, {'사이트': 1681}, {'사이트': 10204}, {'사이트': 2486}, {'사이트': 10070}, {'사이트': 2697}, {'사이트': 3658}, {'사이트': 2185}, {'사이트': 4056}, {'사이트': 10231}, {'사이트': 10052}, {'사이트': 160}, {'사이트': 1614}, {'사이트': 10035}, {'사이트': 10113}, {'사이트': 1573}, {'사이트': 10011}, {'사이트': 10200}, {'사이트': 10191}, {'사이트': 10224}, {'사이트': 10015}, {'사이트': 2350}, {'사이트': 3076}, {'사이트': 10225}, {'사이트': 3059}, {'사이트': 10007}, {'사이트': 10009}, {'사이트': 10223}, {'사이트': 2696}, {'사이트': 2779}, {'사이트': 2676}, {'사이트': 2716}, {'사이트': 1680}, {'사이트': 1337}, {'사이트': 10128}, {'사이트': 10202}, {'사이트': 10274}, {'사이트': 2552}, {'사이트': 10203}, {'사이트': 1725}, {'사이트': 3378}, {'사이트': 1695}, {'사이트': 3196}, {'사이트': 10250}, {'사이트': 1928}, {'사이트': 10061}, {'사이트': 2205}, {'사이트': 10270}, {'사이트': 10154}, {'사이트': 2390}, {'사이트': 10233}, {'사이트': 2533}, {'사이트': 10213}, {'사이트': 1591}, {'사이트': 10254}, {'사이트': 10140}, {'사이트': 1384}, {'사이트': 1698}, {'사이트': 1340}, {'사이트': 10080}, {'사이트': 10053}, {'사이트': 10160}, {'사이트': 1400}, {'사이트': 10029}, {'사이트': 2778}, {'사이트': 1612}, {'사이트': 10186}, {'사이트': 10047}, {'사이트': 10236}, {'사이트': 10257}, {'사이트': 10076}, {'사이트': 1243}, {'사이트': 2343}, {'사이트': 3736}, {'사이트': 10138}, {'사이트': 2423}, {'사이트': 2888}, {'사이트': 3756}, {'사이트': 2419}, {'사이트': 10197}, {'사이트': 2717}, {'사이트': 1360}, {'사이트': 3397}, {'사이트': 2418}, {'사이트': 10262}, {'사이트': 2346}, {'사이트': 4096}, {'사이트': 10041}, {'사이트': 1388}, {'사이트': 3796}, {'사이트': 10130}, {'사이트': 10135}, {'사이트': 1865}, {'사이트': 10068}, {'사이트': 2956}, {'사이트': 10066}, {'사이트': 10260}, {'사이트': 10064}, {'사이트': 3178}, {'사이트': 2487}, {'사이트': 10199}, {'사이트': 1571}, {'사이트': 10127}, {'사이트': 10185}, {'사이트': 3798}, {'사이트': 1570}, {'사이트': 10235}, {'사이트': 2414}, {'사이트': 3339}, {'사이트': 2489}, {'사이트': 2349}, {'사이트': 3357}, {'사이트': 1773}, {'사이트': 10255}, {'사이트': 2511}, {'사이트': 10033}, {'사이트': 2797}, {'사이트': 3616}, {'사이트': 10209}, {'사이트': 10098}, {'사이트': 10115}, {'사이트': 3176}, {'사이트': 3017}, {'사이트': 3258}, {'사이트': 1825}, {'사이트': 2737}, {'사이트': 2206}, {'사이트': 1292}, {'사이트': 10037}, {'사이트': 10178}, {'사이트': 3976}, {'사이트': 3257}, {'사이트': 1926}, {'사이트': 205}, {'사이트': 3797}, {'사이트': 10234}, {'사이트': 3055}, {'사이트': 2339}, {'사이트': 2392}, {'사이트': 10232}, {'사이트': 4044}, {'사이트': 1203}, {'사이트': 4016}, {'사이트': 10207}, {'사이트': 3776}, {'사이트': 3958}, {'사이트': 2005}, {'사이트': 10108}, {'사이트': 10151}, {'사이트': 10057}, {'사이트': 3517}, {'사이트': 2836}, {'사이트': 3959}, {'사이트': 2886}, {'사이트': 1313}, {'사이트': 10049}, {'사이트': 3177}, {'사이트': 3957}, {'사이트': 4048}, {'사이트': 1577}, {'사이트': 2976}, {'사이트': 10072}, {'사이트': 3556}, {'사이트': 790}, {'사이트': 3516}, {'사이트': 3936}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "URL = 'https://card-search.naver.com/graphql'\n",
    "\n",
    "composition_list = []\n",
    "\n",
    "for i in range(1, 30):\n",
    "    params = {\n",
    "        \"operationName\": \"smartSearch\",\n",
    "        \"variables\": {\n",
    "            'basePayment': 0,\n",
    "            'bizType': 'CPC',\n",
    "            'device': 'pc',\n",
    "            'isRefetch': False,\n",
    "            'maxAnnualFee': 0,\n",
    "            'minAnnualFee': 0,\n",
    "            'pageNo': i,\n",
    "            'pageSize': 10,\n",
    "            'sortMethod': 'ri'\n",
    "        },\n",
    "        \"query\": '''query smartSearch($basePayment: Int, $bizType: BizType, $device: AdDeviceType, $isRefetch: Boolean, $maxAnnualFee: Int, $minAnnualFee: Int, $pageNo: Int, $pageSize: Int, $sortMethod: SortMethod) {\n",
    "            cardAdList(\n",
    "                basePayment: $basePayment,\n",
    "                bizType: $bizType,\n",
    "                device: $device,\n",
    "                isRefetch: $isRefetch,\n",
    "                maxAnnualFee: $maxAnnualFee,\n",
    "                minAnnualFee: $minAnnualFee,\n",
    "                pageNo: $pageNo,\n",
    "                pageSize: $pageSize,\n",
    "                sortMethod: $sortMethod\n",
    "            ) {\n",
    "                cardAds {\n",
    "                    cardAdId\n",
    "                    # Add other fields you want to extract here\n",
    "                }\n",
    "                totalSize\n",
    "            }\n",
    "        }'''\n",
    "    }\n",
    "\n",
    "    response = requests.post(URL, json=params, headers=headers)\n",
    "\n",
    "    json_result = response.json()\n",
    "\n",
    "    for data in json_result['data']['cardAdList']['cardAds']:\n",
    "        composition_list.append({\n",
    "            '사이트': data['cardAdId']\n",
    "        })\n",
    "\n",
    "print(composition_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to composition_list.csv.\n"
     ]
    }
   ],
   "source": [
    "#이걸로 parameter 값 뽑기\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "URL = 'https://card-search.naver.com/graphql'\n",
    "\n",
    "composition_list = []\n",
    "\n",
    "for i in range(1, 36):\n",
    "    params = {\n",
    "        \"operationName\": \"smartSearch\",\n",
    "        \"variables\": {\n",
    "            'basePayment': 0,\n",
    "            'bizType': 'CPC',\n",
    "            'device': 'pc',\n",
    "            'isRefetch': False,\n",
    "            'maxAnnualFee': 0,\n",
    "            'minAnnualFee': 0,\n",
    "            'pageNo': i,\n",
    "            'pageSize': 10,\n",
    "            'sortMethod': 'ri'\n",
    "        },\n",
    "        \"query\": '''query smartSearch($basePayment: Int, $bizType: BizType, $device: AdDeviceType, $isRefetch: Boolean, $maxAnnualFee: Int, $minAnnualFee: Int, $pageNo: Int, $pageSize: Int, $sortMethod: SortMethod) {\n",
    "            cardAdList(\n",
    "                basePayment: $basePayment,\n",
    "                bizType: $bizType,\n",
    "                device: $device,\n",
    "                isRefetch: $isRefetch,\n",
    "                maxAnnualFee: $maxAnnualFee,\n",
    "                minAnnualFee: $minAnnualFee,\n",
    "                pageNo: $pageNo,\n",
    "                pageSize: $pageSize,\n",
    "                sortMethod: $sortMethod\n",
    "            ) {\n",
    "                cardAds {\n",
    "                    cardAdId\n",
    "                    # Add other fields you want to extract here\n",
    "                }\n",
    "                totalSize\n",
    "            }\n",
    "        }'''\n",
    "    }\n",
    "\n",
    "    response = requests.post(URL, json=params, headers=headers)\n",
    "\n",
    "    json_result = response.json()\n",
    "\n",
    "    for data in json_result['data']['cardAdList']['cardAds']:\n",
    "        composition_list.append({\n",
    "            '사이트': data['cardAdId']\n",
    "        })\n",
    "\n",
    "# Save the data to a CSV file\n",
    "csv_file = 'composition_list.csv'\n",
    "fieldnames = ['사이트']\n",
    "\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data rows\n",
    "    for row in composition_list:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f'Data has been saved to {csv_file}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'card_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(card_data)\n\u001b[0;32m     40\u001b[0m \u001b[39m# 데이터프레임을 CSV 파일로 저장합니다.\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m df\u001b[39m.\u001b[39;49mto_csv(\u001b[39m'\u001b[39;49m\u001b[39mcard_data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     43\u001b[0m \u001b[39m# 저장 완료 메시지 출력\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m데이터가 card_data.csv 파일로 저장되었습니다.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pooom\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pooom\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3721\u001b[0m     path_or_buf,\n\u001b[0;32m   3722\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3723\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3724\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3725\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3726\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3727\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3728\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3729\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3730\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3731\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3732\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3733\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3734\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3735\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3736\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3737\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pooom\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pooom\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\pooom\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\pooom\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'card_data.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from composition_list.csv to retrieve cardAdId values\n",
    "composition_list = pd.read_csv('composition_list.csv')\n",
    "urls = ['https://card-search.naver.com/item?cardAdId=' + str(card_id) for card_id in composition_list['사이트']]\n",
    "\n",
    "card_data = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        card_names = [card_name.text for card_name in soup.select('b.txt')]\n",
    "        benefit_types = [each.text for each in soup.select('b.text')]\n",
    "        benefit = [bene.text for bene in soup.select('dt.detail_title')]\n",
    "        benefit2 = [bene2.text for bene2 in soup.select('dd.desc')]\n",
    "        # dd class = 'desc'\n",
    "        #app > div > div.BaseInfo > div > div.cardinfo > dl > dd:nth-child(8)\n",
    "        \n",
    "        \n",
    "        # 각 정보를 딕셔너리로 묶어서 리스트에 추가합니다.\n",
    "        for i in range(len(card_names)):\n",
    "            card_info = {\n",
    "                '카드이름': card_names[i],\n",
    "                '혜택종류': benefit_types[i],\n",
    "                '상세혜택': benefit[i],\n",
    "                '상세혜택2': benefit2[i],\n",
    "                \n",
    "            }\n",
    "            card_data.append(card_info)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "    \n",
    "# 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(card_data)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장합니다.\n",
    "df.to_csv('card_data.csv', index=False)\n",
    "\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"데이터가 card_data.csv 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[39m# 각 정보를 딕셔너리로 묶어서 리스트에 추가합니다.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(card_names)):\n\u001b[0;32m     26\u001b[0m         card_info \u001b[39m=\u001b[39m {\n\u001b[0;32m     27\u001b[0m             \u001b[39m'\u001b[39m\u001b[39m카드이름\u001b[39m\u001b[39m'\u001b[39m: card_names[i],\n\u001b[0;32m     28\u001b[0m             \u001b[39m'\u001b[39m\u001b[39m혜택종류\u001b[39m\u001b[39m'\u001b[39m: benefit_types[i],\n\u001b[0;32m     29\u001b[0m             \u001b[39m'\u001b[39m\u001b[39m상세혜택\u001b[39m\u001b[39m'\u001b[39m: benefit[i],\n\u001b[0;32m     30\u001b[0m             \u001b[39m'\u001b[39m\u001b[39m상세혜택2\u001b[39m\u001b[39m'\u001b[39m: benefit2[i],\n\u001b[1;32m---> 31\u001b[0m             \u001b[39m'\u001b[39m\u001b[39m추가항목\u001b[39m\u001b[39m'\u001b[39m: additional_field[i]\n\u001b[0;32m     32\u001b[0m         }\n\u001b[0;32m     33\u001b[0m         card_data\u001b[39m.\u001b[39mappend(card_info)\n\u001b[0;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from composition_list.csv to retrieve cardAdId values\n",
    "composition_list = pd.read_csv('composition_list.csv')\n",
    "urls = ['https://card-search.naver.com/item?cardAdId=' + str(card_id) for card_id in composition_list['사이트']]\n",
    "\n",
    "card_data = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        card_names = [card_name.text for card_name in soup.select('b.txt')]\n",
    "        benefit_types = [each.text for each in soup.select('b.text')]\n",
    "        benefit = [bene.text for bene in soup.select('dt.detail_title')]\n",
    "        benefit2 = [bene2.text for bene2 in soup.select('dd.desc')]\n",
    "        \n",
    "        # Additional field\n",
    "        additional_field = [item.text for item in soup.select('#app > div > div.BaseInfo > div > div.cardinfo > dl > dd:nth-child(8)')]\n",
    "        \n",
    "        # 각 정보를 딕셔너리로 묶어서 리스트에 추가합니다.\n",
    "        for i in range(len(card_names)):\n",
    "            card_info = {\n",
    "                '카드이름': card_names[i],\n",
    "                '혜택종류': benefit_types[i],\n",
    "                '상세혜택': benefit[i],\n",
    "                '상세혜택2': benefit2[i],\n",
    "                '추가항목': additional_field[i]\n",
    "            }\n",
    "            card_data.append(card_info)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "    \n",
    "# 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(card_data)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장합니다.\n",
    "df.to_csv('card_data.csv', index=False)\n",
    "\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"데이터가 card_data.csv 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 card_data.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from composition_list.csv to retrieve cardAdId values\n",
    "composition_list = pd.read_csv('composition_list.csv')\n",
    "urls = ['https://card-search.naver.com/item?cardAdId=' + str(card_id) for card_id in composition_list['사이트']]\n",
    "\n",
    "card_data = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        card_names = [card_name.text for card_name in soup.select('b.txt')]\n",
    "        benefit_types = [each.text for each in soup.select('b.text')]\n",
    "        benefit = [bene.text for bene in soup.select('dt.detail_title')]\n",
    "        benefit2 = [bene2.text for bene2 in soup.select('dd.desc')]\n",
    "        \n",
    "        #추가항목 추가.\n",
    "        additional_field = soup.select('#app > div > div.BaseInfo > div > div.cardinfo > dl > dd:nth-child(8)')\n",
    "        additional_field_value = additional_field[0].text if additional_field else ''  # Check if element exists before accessing text\n",
    "        \n",
    "        #상세\n",
    "        additional_field2 = soup.select('#app > div.cardItem > div.Benefits > div > details:nth-child(2) > div > dl > dd:nth-child(2)')\n",
    "        additional_field_value2 = additional_field2[0].text if additional_field2 else ''\n",
    "\n",
    "        # 각 정보를 딕셔너리로 묶어서 리스트에 추가합니다.\n",
    "        for i in range(len(card_names)):\n",
    "            card_info = {\n",
    "                '카드이름': card_names[i],\n",
    "                '혜택종류': benefit_types[i],\n",
    "                '상세혜택': benefit[i],\n",
    "                '상세혜택2': benefit2[i],\n",
    "                '추가항목': additional_field_value,\n",
    "                '추가항목2' : additional_field_value2\n",
    "                \n",
    "            }\n",
    "            card_data.append(card_info)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "    \n",
    "# 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(card_data)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장합니다.\n",
    "df.to_csv('card_data.csv', index=False)\n",
    "\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"데이터가 card_data.csv 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 card_data.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "#이게 진짜임. navercard 상세항목 크롤링\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from composition_list.csv to retrieve cardAdId values\n",
    "composition_list = pd.read_csv('composition_list.csv')\n",
    "urls = ['https://card-search.naver.com/item?cardAdId=' + str(card_id) for card_id in composition_list['사이트']]\n",
    "\n",
    "card_data = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        card_names = [card_name.text for card_name in soup.select('b.txt')]\n",
    "        benefit_types = [each.text for each in soup.select('b.text')]\n",
    "        benefit = [bene.text for bene in soup.select('dt.detail_title')]\n",
    "        benefit2 = [bene2.text for bene2 in soup.select('dd.desc')]\n",
    "\n",
    "        # 상세1\n",
    "        additional_field = soup.select('#app > div > div.BaseInfo > div > div.cardinfo > dl > dd:nth-child(8)')\n",
    "        additional_field_value = additional_field[0].text if additional_field else ''\n",
    "\n",
    "        # 삼세 2\n",
    "        additional_field2 = soup.select('#app > div.cardItem > div.Benefits > div > details:nth-child(2) > summary > h5 > i')\n",
    "        additional_field_value2 = additional_field2[0].text if additional_field2 else ''\n",
    "\n",
    "        # Additional field 3\n",
    "        additional_field3 = soup.select('#app > div.cardItem > div.Benefits > div > details:nth-child(3) > summary > h5 > i')\n",
    "        additional_field_value3 = additional_field3[0].text if additional_field3 else ''\n",
    "\n",
    "        # Additional field 4\n",
    "        additional_field4 = soup.select('#app > div.cardItem > div.Benefits > div > details:nth-child(4) > summary > h5 > i')\n",
    "        additional_field_value4 = additional_field4[0].text if additional_field4 else ''\n",
    "\n",
    "        # Additional field 5\n",
    "        additional_field5 = soup.select('#app > div.cardItem > div.Benefits > div > details:nth-child(5) > summary > h5 > i')\n",
    "        additional_field_value5 = additional_field5[0].text if additional_field5 else ''\n",
    "\n",
    "        # Additional field 6\n",
    "        additional_field6 = soup.select('#app > div.cardItem > div.Benefits > div > details:nth-child(6) > summary > h5 > i')\n",
    "        additional_field_value6 = additional_field6[0].text if additional_field6 else ''\n",
    "\n",
    "        # Additional field 7\n",
    "        additional_field7 = soup.select('#app > div.cardItem > div.Benefits > div > details:nth-child(7) > summary > h5 > i')\n",
    "        additional_field_value7 = additional_field7[0].text if additional_field7 else ''\n",
    "\n",
    "        # Additional field 8\n",
    "        additional_field8 = soup.select('#app > div.cardItem > div.Benefits > div > details:nth-child(8) > summary > h5 > i')\n",
    "        additional_field_value8 = additional_field8[0].text if additional_field8 else ''\n",
    "\n",
    "        # Additional field 9\n",
    "        additional_field9 = soup.select('#app > div.cardItem > div.Benefits > div > details:nth-child(9) > summary > h5 > i')\n",
    "        additional_field_value9 = additional_field9[0].text if additional_field9 else ''\n",
    "\n",
    "        # 각 정보를 딕셔너리로 묶어서 리스트에 추가합니다.\n",
    "        for i in range(len(card_names)):\n",
    "            card_info = {\n",
    "                '카드이름': card_names[i],\n",
    "                '혜택종류': benefit_types[i],\n",
    "                '상세혜택': benefit[i],\n",
    "                '상세혜택2': benefit2[i],\n",
    "                '추가항목1': additional_field_value,\n",
    "                '추가항목2': additional_field_value2,\n",
    "                '추가항목3': additional_field_value3,\n",
    "                '추가항목4': additional_field_value4,\n",
    "                '추가항목5': additional_field_value5,\n",
    "                '추가항목6': additional_field_value6,\n",
    "                '추가항목7': additional_field_value7,\n",
    "                '추가항목8': additional_field_value8,\n",
    "                '추가항목9': additional_field_value9,\n",
    "            }\n",
    "            card_data.append(card_info)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "    \n",
    "# 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(card_data)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장합니다.\n",
    "df.to_csv('card_data.csv', index=False)\n",
    "\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"데이터가 card_data.csv 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 리펙토링\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from composition_list.csv to retrieve cardAdId values\n",
    "composition_list = pd.read_csv('composition_list.csv')\n",
    "urls = ['https://card-search.naver.com/item?cardAdId=' + str(card_id) for card_id in composition_list['사이트']]\n",
    "\n",
    "# Define additional fields' selectors\n",
    "additional_fields_selectors = [\n",
    "    '#app > div > div.BaseInfo > div > div.cardinfo > dl > dd:nth-child(8)',\n",
    "    '#app > div.cardItem > div.Benefits > div > details:nth-child(2) > summary > h5 > i',\n",
    "    '#app > div.cardItem > div.Benefits > div > details:nth-child(3) > summary > h5 > i',\n",
    "    '#app > div.cardItem > div.Benefits > div > details:nth-child(4) > summary > h5 > i',\n",
    "    '#app > div.cardItem > div.Benefits > div > details:nth-child(5) > summary > h5 > i',\n",
    "    '#app > div.cardItem > div.Benefits > div > details:nth-child(6) > summary > h5 > i',\n",
    "    '#app > div.cardItem > div.Benefits > div > details:nth-child(7) > summary > h5 > i',\n",
    "    '#app > div.cardItem > div.Benefits > div > details:nth-child(8) > summary > h5 > i',\n",
    "    '#app > div.cardItem > div.Benefits > div > details:nth-child(9) > summary > h5 > i'\n",
    "]\n",
    "\n",
    "card_data = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        card_names = [card_name.text for card_name in soup.select('b.txt')]\n",
    "        benefit_types = [each.text for each in soup.select('b.text')]\n",
    "        benefit = [bene.text for bene in soup.select('dt.detail_title')]\n",
    "        benefit2 = [bene2.text for bene2 in soup.select('dd.desc')]\n",
    "        \n",
    "        additional_fields = [soup.select(selector)[0].text if soup.select(selector) else '' for selector in additional_fields_selectors]\n",
    "\n",
    "        # 각 정보를 딕셔너리로 묶어서 리스트에 추가합니다.\n",
    "        for i in range(len(card_names)):\n",
    "            card_info = {\n",
    "                '카드이름': card_names[i],\n",
    "                '혜택종류': benefit_types[i],\n",
    "                '상세혜택': benefit[i],\n",
    "                '상세혜택2': benefit2[i],\n",
    "                '추가항목1': additional_fields[0],\n",
    "                '추가항목2': additional_fields[1],\n",
    "                '추가항목3': additional_fields[2],\n",
    "                '추가항목4': additional_fields[3],\n",
    "                '추가항목5': additional_fields[4],\n",
    "                '추가항목6': additional_fields[5],\n",
    "                '추가항목7': additional_fields[6],\n",
    "                '추가항목8': additional_fields[7],\n",
    "                '추가항목9': additional_fields[8],\n",
    "            }\n",
    "            card_data.append(card_info)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "    \n",
    "# 데이터프레임으로 변환합니다.\n",
    "df = pd.DataFrame(card_data)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장합니다.\n",
    "df.to_csv('card_data.csv', index=False)\n",
    "\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"데이터가 card_data.csv 파일로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
